# Kafka

## What is Kafka

Apache Kafka is a real-time data streaming technology capable of handling trillions of events per day. Initially conceived as a messaging queue, Kafka is based on an abstraction of a distributed commit log. Since being created and open sourced in 2011, Kafka has since become the industry standard for working with data in motion.

Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real-time. Streaming data is data that is continuously generated by thousands of data sources, which typically send the data records in simultaneously. A streaming platform needs to handle this constant influx of data, and process the data sequentially and incrementally.

Technically speaking, event streaming is the practice of capturing data in real-time from event sources like databases, sensors, mobile devices, cloud services, and software applications in the form of streams of events; storing these event streams durably for later retrieval; manipulating, processing, and reacting to the event streams in real-time as well as retrospectively; and routing the event streams to different destination technologies as needed. Event streaming thus ensures a continuous flow and interpretation of data so that the right information is at the right place, at the right time.

## How does it work?

Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications.

Kafka provides three main functions to its users:
- Publish and subscribe to streams of records
- Effectively store streams of records in the order in which records were generated
- Process streams of records in real time

Kafka is primarily used to build real-time streaming data pipelines and applications that adapt to the data streams. It combines messaging, storage, and stream processing to allow storage and analysis of both historical and real-time data.

Kafka is used to build real-time streaming data pipelines and real-time streaming applications. A data pipeline reliably processes and moves data from one system to another, and a streaming application is an application that consumes streams of data. For example, if you want to create a data pipeline that takes in user activity data to track how people use your website in real-time, Kafka would be used to ingest and store streaming data while serving reads for the applications powering the data pipeline. Kafka is also often used as a message broker solution, which is a platform that processes and mediates communication between two applications.

Kafka combines two messaging models, queuing and publish-subscribe, to provide the key benefits of each to consumers. Queuing allows for data processing to be distributed across many consumer instances, making it highly scalable. However, traditional queues aren’t multi-subscriber. The publish-subscribe approach is multi-subscriber, but because every message goes to every subscriber it cannot be used to distribute work across multiple worker processes. Kafka uses a partitioned log model to stitch together these two solutions. A log is an ordered sequence of records, and these logs are broken up into segments, or partitions, that correspond to different subscribers. This means that there can be multiple subscribers to the same topic and each is assigned a partition to allow for higher scalability. Finally, Kafka’s model provides replayability, which allows multiple independent applications reading from data streams to work independently at their own rate.

Kafka can be deployed on bare-metal hardware, virtual machines, and containers, and on-premises as well as in the cloud. You can choose between self-managing your Kafka environments and using fully managed services offered by a variety of vendors.

### Queuing

<img width="570" alt="image" src="https://github.com/user-attachments/assets/37fca941-03f1-44ed-b169-373c6626745d">

### Publish-Subscribe

<img width="568" alt="image" src="https://github.com/user-attachments/assets/a397a923-f76e-4092-932a-1239f1dab334">

## Kafka architecture

### Kafka's multi topic architecture

Kafka remedies the two different models by publishing records to different topics. Each topic has a partitioned log, which is a structured commit log (which consumers can subscribe to) that keeps track of all records in order and appends new ones in real time. These partitions are distributed and replicated across multiple servers, allowing for high scalability, fault-tolerance, and parallelism. Each consumer is assigned a partition in the topic, which allows for multi-subscribers while maintaining the order of the data. By combining these messaging models, Kafka offers the benefits of both. Kafka also acts as a very scalable and fault-tolerant storage system by writing and replicating all data to disk. By default, Kafka keeps data stored on disk until it runs out of space, but the user can also set a retention limit. Kafka has four APIs:

- Admin API: to manage and inspect topics, brokers, and other Kafka objects.

- Producer API: Used to publish a stream of events to a Kafka topic. A topic is a named log that stores the records in the order they occurred relative to one another. After a record is written to a topic, it can’t be altered or deleted; instead, it remains in the topic for a preconfigured amount of time—for example, for two days—or until storage space runs out.

- Consumer API: Used to subscribe to topics and process their streams of events. This enables an application to subscribe to one or more topics and to ingest and process the stream stored in the topic. It can work with records in the topic in real-time, or it can ingest and process past records.

- Streams API: Enables applications to behave as stream processors, which take in an input stream from topic(s) and transform it to an output stream which goes into different output topic(s). To implement stream processing applications and microservices. It provides higher-level functions to process event streams, including transformations, stateful operations like aggregations and joins, windowing, processing based on event-time, and more. Input is read from one or more topics in order to generate output to one or more topics, effectively transforming the input streams to output streams.

- Connector API: The Connector API enables the creation and operation of reusable producers and consumers that link Kafka topics to apps or information systems. For example, links to relational databases may preserve a record of every modification made to tables. Allows users to seamlessly automate the addition of another application or data system to their current Kafka topics. To build and run reusable data import/export connectors that consume (read) or produce (write) streams of events from and to external systems and applications so they can integrate with Kafka. For example, a connector to a relational database like PostgreSQL might capture every change to a set of tables. However, in practice, you typically don't need to implement your own connectors because the Kafka community already provides hundreds of ready-to-use connectors.

### Kafka components

Kafka is a distributed system consisting of servers and clients that communicate via a high-performance TCP network protocol. It can be deployed on bare-metal hardware, virtual machines, and containers in on-premise as well as cloud environments.

<img width="559" alt="image" src="https://github.com/user-attachments/assets/78d90607-5bd0-4fcf-adc6-f6dd83a9cd18">

#### Brokers

A server participating in a Kafka cluster is known as a broker. The Kafka cluster is typically created by numerous brokers cooperating to enable load balancing, trustworthy redundancy, and failover. For cluster administration and coordination, brokers use Apache ZooKeeper.

Each broker instance can handle read and write volumes of up to tens of thousands per second without affecting performance. Each broker has a unique ID and is capable of managing divisions of one or more topic logs. The brokers also use ZooKeeper for a process known as leader elections, where a broker is chosen to take the lead in handling client requests for a particular topic’s unique partition.

If an error or failure occurs with one broker, another broker steps in to perform the functions of the malfunctioning component, and the information is not lost.

#### ZooKeeper

The Kafka cluster is managed and coordinated by Kafka brokers using ZooKeeper. When a Kafka cluster changes, ZooKeeper notifies all nodes. For instance, when a new broker enters the cluster or a broker fails, ZooKeeper notifies the cluster.
Moreover, ZooKeeper facilitates leadership elections among broker and topic partition pairings. It also aids in determining which broker will serve as the lead for each partition and which brokers have identical copies of the data.

Consumers are also informed of the offset value via ZooKeeper.

#### Servers

Kafka is run as a cluster of one or more servers that can span multiple datacenters or cloud regions. Some of these servers form the storage layer, called the brokers. Other servers run Kafka Connect to continuously import and export data as event streams to integrate Kafka with your existing systems such as relational databases as well as other Kafka clusters. To let you implement mission-critical use cases, a Kafka cluster is highly scalable and fault-tolerant: if any of its servers fails, the other servers will take over their work to ensure continuous operations without any data loss.

#### Clients

They allow you to write distributed applications and microservices that read, write, and process streams of events in parallel, at scale, and in a fault-tolerant manner even in the case of network problems or machine failures. Kafka ships with some such clients included, which are augmented by dozens of clients provided by the Kafka community: clients are available for Java and Scala including the higher-level Kafka Streams library, for Go, Python, C/C++, and many other programming languages as well as REST APIs.

#### Events

An event records the fact that "something happened" in the world or in your business. It is also called record or message in the documentation. When you read or write data to Kafka, you do this in the form of events. Conceptually, an event has a key, value, timestamp, and optional metadata headers. Here's an example event:

- Event key: "Alice"
- Event value: "Made a payment of $200 to Bob"
- Event timestamp: "Jun. 25, 2020 at 2:06 p.m."

#### Producers and consumers

Producers are those client applications that publish (write) events to Kafka, and consumers are those that subscribe to (read and process) these events. In Kafka, producers and consumers are fully decoupled and agnostic of each other, which is a key design element to achieve the high scalability that Kafka is known for. For example, producers never need to wait for consumers. Kafka provides various guarantees such as the ability to process events exactly-once.

A producer can also determine which partition a specific record or message is published to because partitions are utilized to offer further scalability. Producers do not need to define any particular partition; consequently, it is possible to load balance topics in a round-robin fashion.

The Kafka partition is offset because Kafka brokers are stateless. The consumer keeps track of how many messages have been consumed. Additionally, once the consumer recognizes a specific message offset, you can be sure that they have consumed all previous messages. The consumer sends asynchronous pull requests to the broker to provide a buffer of bytes ready for consumption. Users can fast-forward to any point in a partition by simply providing an offset value. Consumers are also informed of the offset value via ZooKeeper.

#### Topics

Events are organized and durably stored in topics. Very simplified, a topic is similar to a folder in a filesystem, and the events are the files in that folder. An example topic name could be "payments". Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, as well as zero, one, or many consumers that subscribe to these events. Events in a topic can be read as often as needed—unlike traditional messaging systems, events are not deleted after consumption. Instead, you define for how long Kafka should retain your events through a per-topic configuration setting, after which old events will be discarded. Kafka's performance is effectively constant with respect to data size, so storing data for a long time is perfectly fine.

Topics are partitioned, meaning a topic is spread over a number of "buckets" located on different Kafka brokers. This distributed placement of your data is very important for scalability because it allows client applications to both read and write the data from/to many brokers at the same time. When a new event is published to a topic, it is actually appended to one of the topic's partitions. Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, and Kafka guarantees that any consumer of a given topic-partition will always read that partition's events in exactly the same order as they were written.

<img width="453" alt="image" src="https://github.com/user-attachments/assets/e8f96fc1-319a-4f88-b288-03e0ce52cdb4">
Figure: This example topic has four partitions P1–P4. Two different producer clients are publishing, independently from each other, new events to the topic by writing events over the network to the topic's partitions. Events with the same key (denoted by their color in the figure) are written to the same partition. Note that both producers can write to the same partition if appropriate.

To make your data fault-tolerant and highly-available, every topic can be replicated among brokers, even across geo-regions or datacenters, so that there are always multiple brokers that have a copy of the data just in case things go wrong, you want to do maintenance on the brokers, and so on. A common production setting is a replication factor of 3, i.e., there will always be three copies of your data. This replication is performed at the level of topic-partitions.

Kafka guarantees that all communications within a partition are organized in chronological order. You can identify a communication by studying its offset, similar to a standard array index. This offset is a sequence number that is increased for each new message in a partition. A topic can be read in parallel by many consumers from each partition.

#### Consumer group

Consumers who share a task or are related comprise a Kafka consumer group. Kafka distributes messages from topic partitions to group members that are consumers. Each section is read just once by a single consumer in the group at the moment it is read. A consumer group can run numerous processes or instances concurrently and has a distinct group-id. One user can read from a singular partition for each consumer group with several partitions. 

## Kafka use cases

### Data pipelines

In the context of Apache Kafka, a streaming data pipeline means ingesting the data from sources into Kafka as it’s created, and then streaming that data from Kafka to one or more targets. This allows for seamless data integration and efficient data flow across different systems.

### Stream processing

Stream processing includes operations like filters, joins, maps, aggregations, and other transformations that enterprises leverage to power many use cases. Kafka Streams, a stream processing library built for Apache Kafka, enables enterprises to process data in real-time, making it ideal for applications requiring immediate data processing and analysis.

### Streaming analytics

Kafka provides high throughput event delivery. When combined with open-source technologies such as Druid, it can form a powerful Streaming Analytics Manager (SAM). Druid consumes streaming data from Kafka to enable analytical queries. Events are first loaded into Kafka, where they are buffered in Kafka brokers, then they are consumed by Druid real-time workers. This allows for real-time analytics and decision-making.

### Event-driven microservices

Apache Kafka is the most popular tool for microservices, because it solves many issues related to microservices orchestration, while enabling attributes that microservices aim to achieve, such as scalability, efficiency, and speed. Kafka also facilitates inter-service communication, preserving ultra-low latency and fault tolerance. This makes it essential for building robust and scalable microservices architectures.

By using Kafka's capabilities, organizations can build highly efficient data pipelines, process streams of data in real time, perform advanced analytics, and develop scalable microservices—all ensuring they can meet the demands of modern data-driven applications.

## Kafka vs other services

Kafka, has a reasonably low overhead compared to other messaging systems since it does not monitor user activity or remove messages that have been read. On the other hand, it keeps all messages for a predetermined period and leaves it up to the user to track which messages have been read. Each node in a Kafka cluster is referred to as a broker, and the Kafka software operates on one or more servers.

Kafka utilizes an open-source server called Apache ZooKeeper to manage clusters. To make topics more manageable, they are separated into partitions, and Kafka ensures strong ordering for every partition. In 2011, Kafka (initially created at LinkedIn) was made publicly available. 

Kafka integrates publish-subscribe and queuing messaging technologies to give users the key advantages of each. Queuing is very scalable because it enables the distribution of data processing across numerous consumer instances. Traditional queues, however, are not multi-subscriber compatible. Even though the publish-subscribe method can have more than one subscriber, one cannot use it to divide work between different worker processes. This is because every message is sent to every subscriber.

### Kafka vs SQS

![image](https://github.com/user-attachments/assets/6441b630-54d4-4d40-a3e3-f8f5d979d4e7)

When to choose SQS?

When you need a simple, managed, scalable queue for decoupling components or background task processing.

When to choose Kafka?

When you need a high-throughput, distributed event-streaming platform for real-time analytics, data pipelines, or event-driven architectures.

### Kafka vs RabbitMQ

RabbitMQ is an open source message broker that uses a messaging queue approach. Queues are spread across a cluster of nodes and optionally replicated, with each message only being delivered to a single consumer.

<img width="652" alt="image" src="https://github.com/user-attachments/assets/e15931b1-82df-4e80-91bf-c14d78b6f7d8">

Because Kafka began as a kind of message broker (and can, in theory, still be used as one) and because RabbitMQ supports a publish/subscribe messaging model (among others), Kafka and RabbitMQ are often compared as alternatives. But, the comparisons aren’t really practical, and they often dive into technical details that are beside the point when choosing between the two. For example, Kafka topics can have multiple subscribers, whereas each RabbitMQ message can have only one; or Kafka topics are durable, whereas RabbitMQ messages are deleted once consumed.

- Kafka is a stream-processing platform that enables applications to publish, consume and process high volumes of record streams in a fast and durable way; and

- RabbitMQ is a message broker that enables applications that use different messaging protocols to send messages to, and receive messages from, one another.

## Advantages

### Throughput

Deliver messages at network limited throughput using a cluster of machines with latencies as low as 2ms.

Kafka can handle many more consumers with little impact on throughput because it doesn’t keep track of data acknowledgments and messages from each consumer application. Many applications even adopt a batch user-style in production, when a consumer receives all the messages in a queue at regular intervals.

### Scalability

Scale production clusters up to a thousand brokers, trillions of messages per day, petabytes of data, hundreds of thousands of partitions. Elastically expand and contract storage and processing.

### Permanent storage

Store streams of data safely in a distributed, durable, fault-tolerant cluster.

Partitions are distributed and replicated across many servers, and the data is all written to disk. This helps protect against server failure, making the data very fault-tolerant and durable. 

Kafka provides durable storage by abstracting the distributed commit log commonly found in distributed databases. This makes Kafka capable of acting as a “source of truth,” able to distribute data across multiple nodes for a highly available deployment, whether within a single data center or across multiple availability zones. This durable and persistent storage ensures data integrity and reliability, even during server failures.

### Event streaming

Process streams of events with joins, aggregations, filters, transformations, and more, using event-time and exactly-once processing.

### Integrations

Kafka’s out-of-the-box Connect interface integrates with hundreds of event sources and event sinks including Postgres, JMS, Elasticsearch, AWS S3, and more.

### Client libraries

Read, write, and process streams of events in a vast array of programming languages.

Large ecosystem of open source tools: Leverage a vast array of community-driven tooling.

### Low latency

Kafka can deliver a high volume of messages using a cluster of machines with latencies as low as 2ms. This low latency is crucial for applications that require real-time data processing and immediate responses to data streams.

## Disadvantages

### Kafka doesn't provide a complete set of monitoring tools

Apache Kafka does not contain a complete set of monitoring as well as managing tools. Thus, new startups or enterprises fear to work with Kafka.

### Message tweaking issues

The Kafka broker uses system calls to deliver messages to the consumer. In case, the message needs some tweaking, the performance of Kafka gets significantly reduced. So, it works well if the message does not need to change.

### Kafka does not support wildcard topic selection

Apache Kafka does not support wildcard topic selection. Instead, it matches only the exact topic name. It is because selecting wildcard topics make it incapable to address certain use cases.

### Reduces performance by compressing data

Brokers and consumers reduce the performance of Kafka by compressing and decompressing the data flow. This not only affects its performance but also affects its throughput.

### Complexity

Setting and deploying Kafka can be complex. Additionally, maintaining Kafka in a distributed system can be complex. Kafka is more suitable where the data has a high throughput, requires low latency and needs a persistent storage.

### When not to use Kafka

#### When working with little data

As Kafka is designed to handle high volumes of data, it’s overkill if you need to process only a small amount of messages per day (up to several thousand).  Use traditional message queues such as RabbitMQ for relatively smaller data sets or as a dedicated task queue.

#### Streaming ETL

Despite the fact that Kafka has a stream API, it’s painful to perform data transformations on the fly.  It requires that you build a complex pipeline of interactions between producers and consumers and then maintain the entire system. This requires substantial work and effort and adds complexity.  It’s best to avoid using Kafka as the processing engine for ETL jobs, especially where real-time processing is needed.  That said, there are third-party tools you can use that work with Kafka to give you additional robust capabilities – for example, to optimize tables for real-time analytics.

## Kafka deployment and setup
